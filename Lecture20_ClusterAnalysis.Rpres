Cluster Analysis
========================================================
autosize: true
width: 1600
height: 1000
Guest Lecturer: Joe Tuccillo

GEOG 4023/5023 - Quantitative Methods  
Spring 2020

<style>
.small-code pre code {
  font-size: 1em;
}
</style>

```{r setup, include = FALSE}
library(rgdal)
library(sf)
library(cluster)
library(RColorBrewer)
library(ggplot2)
library(reshape2)
library(knitr)
library(FactoMineR)
# library(cowplot)
source('fastClusterGVF.R') # custom goodness of fit measure

## load example data - Denver census tracts in 2012
den = readOGR(getwd(), 'denver12', verbose = F)
den = den[den$GEOID!='08031980100',] # remove unpopulated tract
```

Introduction
===
- Classification involves using data to group places, events, things into types.
- In geography we are often interested classifying places into regions. 
- A "gentrifying neighborhood” evokes a multidimensional picture of a place. 

Cluster Analysis vs. an Atlas
===
- Traditionally geographers would explore multivariate spatial data by producing an atlas. 
- Each page of the atlas would describe a single variable (e.g. income, homeownership, third-wave coffee shop density, dog salons per capita...).
- We could to identify “gentrifying” neighborhoods by cross referencing maps.

Cluster Analysis vs. an Atlas
===
- However, as our data grew in complexity the task of identifying interesting patterns in the data would become more complex. 
- If we had 50 variables we would need to have an atlas with at least 50 pages.
- We would have to cross reference many maps. Identifying patterns would be difficult.

Cluster Analysis vs. an Atlas
===
- Cluster Analysis is a set of statistical techniques to let people “discover” groups in data. 
- What you could do visually in 2 or 3 dimensions cluster analysis does in high dimensions.
- Each data point is labeled by group membership. 
- Each cluster has an **attribute profile**...kind of like a "bookmarked" set of atlas pages that best describe places in that cluster.

Example
===
<center>
```{r, echo = FALSE}
knitr::include_graphics('figs/singleton_spielman_ex.jpeg')
```
<font size="4"><center>(Singleton and Spielman 2015)</center></font>
</center>

Geographic Cluster Analysis
===
- Cluster analysis joins areas that are close together in “attribute space".
- We might want to add a spatial constraint to this process.
- Cluster analysis with a spatial constraint is sometimes called **regionalization**.

Regionalization Example
===
<center>
```{r, echo = FALSE, out.width = '50%'}
knitr::include_graphics('figs/ecoregions.png')
```
</center>

Space-Time Clustering
===
<center>
We can also use cluster analysis to describe how places change over time: 
```{r, echo = FALSE}
knitr::include_graphics('figs/02_transitions.png')
```

</center>

Space-Time Clustering
===
<center>
We can also use cluster analysis to describe how places change over time: 
```{r, echo = FALSE}
knitr::include_graphics('figs/03_processes.png')
```
</center>

Clusters in Attribute Space
===

- Clusters are sometimes visually obvious in high-density regions of the "attribute space".
- High-density regions = lots of mutual nearest-neighbors.
- We can also describe each cluster's members as being closer to a "central" or "exemplary" observation than any other. 

***

```{r, echo = FALSE}
## create data 
set.seed(808)
a <- data.frame(rnorm(500, mean = 0.5, sd = 0.2) + 5,
rnorm(500) +5)
b <- data.frame(rnorm(500, mean = 0.2, sd = 0.3) + 1,
rnorm(500) +2)
c <- data.frame(rnorm(500, mean = 0.1, sd = 0.25) + 3,
rnorm(500) -3)

## draw groups
plot(a, xlim=c(-2, 9), ylim=c(-7,
9), pch=16, col=4, 
xlab = 'Attribute A', ylab = 'Attribute B', 
main = 'Clustering in a Perfect World')
points(b, col=2, pch=16)
points(c, col=3, pch=16)
```

Clustering Around Centroids: K-Means
===

1. Randomly place *k* number of points (centroids) in the attribute space. 
2. Each dot (observation) is assigned to the nearest centroid.
3. Centroid is recalculated as the mean center of the points assigned to it. 
4. Repeat steps 2 - 3 until "convergence" (centroids stop moving).

***

```{r, echo = FALSE, out.width = '50%'}
knitr::include_graphics('figs/Kmeans_animation.gif')
```

K-Means Example on "Real World" Data
===

```{r, eval = F, include = F}
# run this chunk to display the variable descriptions
View(read.csv('Denver_Variables.csv'))
```

```{r}
## remove geographic identifiers, x/y coords, 
## and test variable (health insurance coverage) from cluster inputs
clust_vars = names(den)[!names(den) %in% c('GEOID', 'Nghbrhd', 'id_nbh', 'INTPTLA', 'INTPTLO', 'Hlth_Un', 'Hlth_In')]

## generate clustering input dataframe
clust_dat = den@data[,clust_vars]

## standardize (z-score) cluster inputs
clust_dat_z = scale(clust_dat)

## perform k-means clustering and extract group labels
set.seed(909)
kmclust = kmeans(clust_dat_z, centers = 7)$cluster 
```

***

```{r, echo = FALSE}
## map the clusters in physical space
den_map = as(den,'sf')
den_map$kmeans_cluster = factor(kmclust)
plot(den_map['kmeans_cluster'],main = paste('Denver Tracts 2012: k-means with k=7 clusters'))
```

Clustering by (Dis)similarity
===
- A **dissimilarity matrix** is like a "mileage chart" for our data.
- An **affinity matrix** is like the inverse of a mileage chart (proximity, nearest neighbors).

*** 

```{r, echo = FALSE}
knitr::include_graphics('figs/dist_ex.png')
```

Clustering by (Dis)similarity
===
- The choice of dissimilarity vs. affinity depends on the clustering method.
  - Some methods work to _distinguish features_ in the data (dissimilarity). 
  - Other methods work to find _mutual similarities_ (affinity).  

*** 

```{r, echo = FALSE}
knitr::include_graphics('figs/dist_ex.png')
```


Clustering by (Dis)similarity
===
- Dissimilarity/affinity matrices can be built from: 
    - **Continuous** data (i.e. Euclidean or Manhattan distance)
    - **Categorical** data (i.e. Matching or Jaccard distance)
    - **Mixed-type** data (i.e. Gower distance)
    
Hierarchical Clustering 
===
- Use dissimilarities to organize data into a **dendrogram**, a tree-like diagram.
  - The "leaves" describe individual observations.
  - The "stems" describe more specific clusters.
  - The "branches" describe more general clusters.
- Clusters are found by "cutting" the dendrogram at a desired height of $k$. 
  
***
```{r, echo = FALSE}
knitr::include_graphics('figs/WPGMA_Dendrogram_5S_data.svg')
```


Hierarchical Clustering: Linking/Sorting Strategies
===
- Distance-based: merge the groups *closest* on some criterion
  - **Single linkage** (nearest-neighbor)
  - **Complete linkage** (furthest-neighbor)
  - **Average linkage** (centroid)
- Variance-based: **Ward's Method**
  - Each merge minimizes the "error sum of squares" (ESS), a measure of within-cluster dissimilarity.
  
***
```{r, echo = FALSE}
knitr::include_graphics('figs/WPGMA_Dendrogram_5S_data.svg')
```

  
A Dendrogram of Denver Tracts
===
- Generate a euclidean distance matrix using R's `dist` function.
- Generate a dendrogram from the distance matrix using R's `hclust` function.
- Use the _Ward_ linkage method (minimize within-cluster variance).

***
```{r}
## add neighborhood ids to rownames for readability
rownames(clust_dat_z) = den$id_nbh

## generate distance matrix
d = dist(clust_dat_z)

## ward dendrogram
dend = hclust(d = d, method = 'ward.D2')
```

A Dendrogram of Denver Tracts
===
<center>
```{r, echo = FALSE, fig.height = 10, fig.width = 20}
## view the dendrogram 
plot(dend)
```
</center>

Finding a Suitable k
===
- We still need to identify a suitable number of clusters $k$ at which to cut the dendrogram.
- Clusters should be **internally consistent** and **well separated**.
- Often useful to consider variance-based and separation-based criteria.
- Can iterate *k* groups and measure each of these criteria to find a "best" solution.

Finding a Suitable k: Variance-Based Criteria
===
<center>
$$GVF = \frac{BSS}{TSS}$$

where 

$$TSS = BSS + WSS$$
</center>

***

Sometimes called **Goodness of Variance Fit (GVF)**.
- **Total Sum of Squares (TSS)**: sum of squared distances from data centroid
- **Between Sum of Squares (BSS)**: sum of squared distances between all group centroids and the data centroid
- **Within Sum of Squares (WSS)**: sum of squared distances between all group members and their centroid

Finding a Suitable k: Variance-Based Criteria
===
<center>
$$GVF = \frac{BSS}{TSS}$$

where 

$$TSS = BSS + WSS$$
</center>

***

Look familiar? 
- This is kind of like an R^2 statistic for clustering.
- Instead of a "line of best fit", we have cluster centroids.
- Instead of "fitted values", we have cluster labels.
- The "explained" part (BSS) tells us how distinct from data centroid the cluster centroids are.
- The "unexplained" part (WSS) tells us how much observations vary about their own centroid.

Finding a Suitable k: Separation-Based Criteria
===
### Information provided by each cluster should be distinct, and shared as little as possible with other clusters.

In other words, clusters should be **well separated**. cases should be grouped:
  - _With_ others _like them_,
  - _Apart from_ those _unlike them_.

Finding a Suitable k: Separation-Based Criteria
===
- **Average Silhouette Width:** to what degree do clusters overlap?
  - 0 = high overlap
  - 1 = high separation
  - -1 = incorrect labels.

Selecting k for Denver Tracts
===
1. Specify a desired range of cluster numbers (here $k=3...10$).
2. Cut the dendogram at each `k` and...
  - Compute variance-based measure (Goodness of Variance Fit).
  - Compute separation-based measure (Average Silhouette Width in R library `cluster`).
4. Compare the measures visually and identify an optimal solution.

***

```{r}
## Specify a range of cluster numbers
krange = 3:10

# goodness of variance fit
gvf = sapply(krange, function(k){
  
  kclust = cutree(dend, k)
  fastClusterGVF(dat = clust_dat, clust = kclust)
  
})

# average silhouette width
sil = sapply(krange, function(k){
  
  kclust = cutree(dend, k)
  mean(silhouette(x = kclust, dist = d)[,3])
  
})
```

Selecting k for Denver Tracts
===

<center>
```{r, echo = FALSE, fig.width = 12}
## output diagnostics 
cl_diag = cbind(k = krange, gvf = gvf, sil = sil)
par(mfrow=c(1,2))
plot(gvf ~ k, data = cl_diag, type='l', main = 'Goodness of Variance Fit')
plot(sil ~ k, data = cl_diag, type='l', main = 'Average Silhouette Width')
```
</center>

***

<left>
```{r, echo = FALSE}
kable(round(data.frame(cl_diag), 2))
```
</left>

Map the Clusters
===
```{r}
## cut dendrogram at best k
best_k = 7

## assign it to the map data
den_map['cluster'] = factor(cutree(dend, best_k))
```

```r
## plot the map
plot(den_map['cluster'],main = paste('Denver Tracts 2012: k =',best_k,'clusters'))
```

***

```{r echo = F}
## plot the map
plot(den_map['cluster'],main = paste('Denver Tracts 2012: k =',best_k,'clusters'))
```

Examining the Clusters
===
- How well do the clusters match with prior expectations of data under analysis?
  - e.g. Tracts in Downtown Denver are probably distinct from the rest of the city.
- How well do the clusters suit the objectives of the analysis? 
  - e.g. Revealing different "socially vulnerable" neighborhood types.
  
Examining the Clusters
===
- **External measures** or a **"ground-truth"** can be useful.
- Do the clusters align well with a particular outcome?
  - e.g. Are crime events concentrated more heavily in one cluster than others?
- Do the clusters match a "ground truth" well?
  - e.g. prior findings from a field assessment.

Comparing the Denver Tract Clusters to an External Measure
===
<center>
```{r, echo = FALSE, fig.align = 'right'}
## re-plot map
plot(den_map['cluster'],main = paste('Denver Tracts 2012: k =',best_k,'clusters'))
```

***

```{r, echo = FALSE, fig.aline = 'left'}
## assign cluster labels to full data
den@data$cluster = factor(cutree(dend, best_k))

## sort clusters by median of outcome variable, highest to lowest
health_uninsured_by_cluster = with(den@data, aggregate(Hlth_Un ~ cluster, FUN = 'median'))
health_uninsured_by_cluster = health_uninsured_by_cluster[order(health_uninsured_by_cluster$Hlth_Un, decreasing = T),]
den$cluster = factor(den$cluster, levels = health_uninsured_by_cluster$cluster)

## plot outcome variable
cols = brewer.pal(best_k, 'Set2')[as.numeric(levels(den$cluster))]
with(den@data, plot(Hlth_Un ~ cluster, col = cols, main = 'Percent of Population without Health Insurance by Cluster', ylab = 'Percent Uninsured'))
abline(h = median(den$Hlth_Un), col = 'red')
```
</center>

Profiling Clusters
===
<center>
An **average profile** plot tells us how far above/below the data mean a cluster is on the input variables:  

```{r, echo = FALSE}
## generate average profile of a cluster
den_cent = colMeans(clust_dat) # the data centroid
cl = levels(den$cluster)[1] # our target cluster
cl_cent = colMeans(clust_dat[den$cluster==cl,]) # the cluster centroid
avp = (cl_cent - den_cent) / den_cent # difference from data mean
avp = avp[avp > 0] # limit to variables above the Denver mean

## format the data for plotting
avp_melt = melt(avp)
var_order = rownames(avp_melt)[order(avp_melt$value, decreasing = T)] # plot vars greatest to smallest
avp_melt$variable = factor(rownames(avp_melt), levels = var_order) 
cl_col = cols[which(levels(den$cluster) == cl)] # for coloring barplot

ggplot(avp_melt, aes(x = variable, y = round(100 * value))) + 
  geom_bar(fill = cl_col, color = 'black', stat = 'identity') +
  xlab('Variable') + 
  ylab ('Percent above Denver Mean') +
  theme_bw() + 
  theme(axis.text.x=element_text(angle=45,hjust=1, size = 12)) +
  ggtitle(paste('Average Profile for Cluster',cl))
```
</center>

Clustering with Principal Components Analylsis (PCA)
===
- PCA performs a linear transformation of the data based on covariances among the input features.
- In the "PCA space", cases are organized around **latent constructs** (i.e., "gentrification", "hardship", "aging") represented by each PCA dimension. 
- Clustering is performed on these latent constructs rather than directly on the input features.

PCA for Denver Tracts
===
```{r, echo = FALSE, fig.height = 10, fig.width = 10}
## build the PCA inputs
pca_in <- den@data[,c('Hlth_Un', clust_vars)] # we will impose the 'no health insurance' variable in the PCA space
rownames(pca_in) <- den$id_nbh # assign neighborhood IDs

# perform PCA (z-scoring handled internally)
# for simplicity's sake, only keep the first 2 components
pca <- PCA(pca_in, scale.unit = T, quanti.sup = 1,  ncp = 2, graph = F) 

## plot PCA loadings
plot(pca, choix = 'var', cex = 2)
```
***
- **Dimension 1: Socioeconomic Status** 
Above Poverty/Homeowners -> Below Poverty/Renters
- **Dimension 2: Youth/Domesticity** 
via Race/Ethnicity, SES, Housing Density.

PCA for Denver Tracts
===
```{r, echo = FALSE, fig.height = 10, fig.width = 10}
plot(pca, choix = 'var', cex = 1.5)
```

***

```{r echo = FALSE, fig.height = 12, fig.width = 12}
plot(pca, choix = 'ind', labels = den$id_nbh, cex = 0.7)
```



Clustering in PCA Space
===
1. Extract PCA scores (projections of the Denver tracts). 
2. Compute Euclidean distances on the scaled PCA scores.
3. Perform Ward Hierarchical Clustering using $k$ = 7 clusters again (for comparison's sake).

***
```{r}
## extract PCA scores (projections of each case)
pca.scores <- pca$ind$coord %>% 
              scale %>% 
              data.frame

## cluster using Ward's method
clust_pca <- pca.scores %>% 
             dist %>% 
             hclust(method = 'ward.D2') %>% 
             cutree(k = 7) %>% 
             factor # for plotting
```

Clustering in PCA Space
===
```{r echo = F}
## assign clusters to map data
den_map['clust_pca'] <- factor(clust_pca)

## make plots
plt_pca <- plot(pca, choix = 'var') +
 geom_point(data = pca.scores[-100,], # omit Obs 100/Sun Valley (outlier)
           aes(x = Dim.1,
               y = Dim.2,
               color = clust_pca[-100]),
           size = 5, alpha = 0.4,
           inherit.aes = F) +
  scale_color_brewer(type = 'qual', palette = 'Set2') + 
  labs(color = 'Cluster')

map_pca <- ggplot(data = den_map, aes(fill = clust_pca)) + 
  geom_sf(size = 0.1, color = 'black') + 
  scale_fill_brewer(type = 'qual', palette = 'Set2') +
  ggtitle(paste('Denver Tracts 2012: k =',best_k,'clusters')) + 
  theme_void() +
  theme(legend.position = 'none',
        plot.title = element_text(hjust = 0.5))
```

```{r echo = F, fig.height = 8, fig.width = 10}
map_pca
```

***

```{r echo = F, fig.height = 10, fig.width = 10}
plt_pca
```


To PC or not to PC?
===
- When clustering, **use PCA with caution!**
- PCA is useful to develop a "high level" understanding of the data...
- ...but the PCA scores can also oversimplify relationships among places.  

More Clustering Methods
===
### Some methods handle selection of $k$ themselves...
  - **Density: Mutual Neighbors** (Community Detection)
  - **Density: Radius Neighbors** (DBSCAN)
  - **Around _Exemplars_** (Affinity Propagation) 
  
_Tradeoff_: Sometimes these methods generate a lot of unclassified observations or "noise".


More Clustering Methods
===
### What if I have categorical data?
- Use **Model-Based** clustering techniques like **Latent Class Analysis (LCA)**!
- Model parameters are determined using maximum likelihood (similar to GLMs).
- Information-theoretic criteria (i.e. AIC, BIC) are used to find a suitable $k$.

More Clustering Methods
===
- Model-based clustering techniques also allow **multiple memberships** ("fuzzy" clusters)
- Availble for continuous data (`mclust`, `teigen`) and categorical data (`poLCA`, `bayesLCA`)

More Data Reduction Techniques
===
- **Multidimensional Scaling (MDS)** for _dissimilarities_.
- **Graph Decomposition (Spectral Analysis)** for _affinities_ or _nearest neighbors_.
- **Homogeneity Analysis/MCA** for _categorical data_.

Conclusion
===
#### Clustering is super flexible! There is no "one size fits all" approach. 

**Always design a cluster analysis with your own research aims in mind!**
  - Variable selection
  - Representation of data (i.e., data reduction, transformation)
  - Clustering methods and their objectives
  - Cluster validation/interpretation